{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as pp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tp\n",
    "\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format = 'svg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coin flip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials = [0, 1, 2, 3, 4, 5, 8, 15, 50, 500, 1000, 2000]\n",
    "data = tp.distributions.Bernoulli(probs=0.5).sample(trials[-1])\n",
    "sums = [tf.reduce_sum(data[:trial]) for trial in trials]\n",
    "\n",
    "prior_alpha = 1\n",
    "prior_beta = 1\n",
    "\n",
    "points = tf.linspace(start=0.0, stop=1.0, num=1000, name='linspace')\n",
    "posterior_densities = [\n",
    "    tp.distributions.Beta(concentration1=tf.to_float(prior_alpha + sum),\n",
    "                          concentration0=tf.to_float(prior_beta + trial - sum))\n",
    "    .prob(points)\n",
    "    for trial, sum in zip(trials, sums)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sums_stacked = tf.stack(sums)\n",
    "posterior_densities_stacked = tf.stack(posterior_densities)\n",
    "\n",
    "session = tf.Session()\n",
    "\n",
    "[\n",
    "    data_,\n",
    "    sums_stacked_,\n",
    "    points_,\n",
    "    posterior_densities_stacked_,\n",
    "] = session.run([\n",
    "    data,\n",
    "    sums_stacked,\n",
    "    points,\n",
    "    posterior_densities_stacked,\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.figure(figsize=(10, 12))\n",
    "for i in range(len(trials)):\n",
    "    axis = pp.subplot(len(trials) / 2, 2, i + 1)\n",
    "    pp.setp(axis.get_yticklabels(), visible=False)\n",
    "    pp.plot(points_, posterior_densities_stacked_[i], \n",
    "            label='%d tosses with %d heads' % (trials[i], sums_stacked_[i]))\n",
    "    pp.fill_between(points_, 0, posterior_densities_stacked_[i], alpha=0.4)\n",
    "    pp.axvline(x=0.5, color='black', linestyle='--', lw=1)\n",
    "    pp.legend().get_frame().set_alpha(0.4)\n",
    "    pp.autoscale(tight=True)\n",
    "pp.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    13, 24, 8,  24,  7, 35, 14, 11, 15, 11, 22, 22, 11, 57,\n",
    "    11, 19, 29,  6, 19, 12, 22, 12, 18, 72, 32,  9,  7, 13,\n",
    "    19, 23, 27, 20,  6, 17, 13, 10, 14,  6, 16, 15,  7,  2,\n",
    "    15, 15, 19, 70, 49,  7, 53, 22, 21, 31, 19, 11, 18, 20,\n",
    "    12, 35, 17, 23, 17,  4,  2, 31, 30, 13, 27,  0, 39, 37,\n",
    "    5,  14, 13, 22,\n",
    "]\n",
    "observation_num = len(data)\n",
    "data = tf.constant(data, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unnormalized_log_probability(data, lambda_1, lambda_2, tau):\n",
    "    alpha = 1.0 / tf.reduce_mean(data)\n",
    "    rv_lambda_1 = tp.distributions.Exponential(rate=alpha)\n",
    "    rv_lambda_2 = tp.distributions.Exponential(rate=alpha)\n",
    "    rv_tau = tp.distributions.Uniform()\n",
    "    indices = tf.range(tf.size(data))\n",
    "    indices = tf.to_int32(tau * tf.to_float(tf.size(data)) <= tf.to_float(indices))\n",
    "    lambda_12 = tf.gather([lambda_1, lambda_2], indices=indices)\n",
    "    rv_observation = tp.distributions.Poisson(rate=lambda_12)\n",
    "    return (\n",
    "        rv_lambda_1.log_prob(lambda_1) +\n",
    "        rv_lambda_2.log_prob(lambda_2) +\n",
    "        rv_tau.log_prob(tau) +\n",
    "        tf.reduce_sum(rv_observation.log_prob(data))\n",
    "    )\n",
    "\n",
    "state = [\n",
    "    tf.reduce_mean(data),\n",
    "    tf.reduce_mean(data),\n",
    "    0.5,\n",
    "]\n",
    "\n",
    "bijector = [\n",
    "    tp.bijectors.Exp(),\n",
    "    tp.bijectors.Exp(),\n",
    "    tp.bijectors.Sigmoid(), \n",
    "]\n",
    "\n",
    "with tf.variable_scope(tf.get_variable_scope(), reuse=tf.AUTO_REUSE):\n",
    "    step_size = tf.get_variable(name='step_size',\n",
    "                                initializer=tf.constant(0.05),\n",
    "                                trainable=False,\n",
    "                                use_resource=True)\n",
    "\n",
    "kernel = tp.mcmc.TransformedTransitionKernel(\n",
    "    inner_kernel=tp.mcmc.HamiltonianMonteCarlo(\n",
    "        target_log_prob_fn=partial(unnormalized_log_probability, data),\n",
    "        num_leapfrog_steps=2,\n",
    "        step_size=step_size,\n",
    "        step_size_update_fn=tp.mcmc.make_simple_step_size_update_policy(),\n",
    "        state_gradients_are_stopped=True,\n",
    "    ),\n",
    "    bijector=bijector,\n",
    ")\n",
    "\n",
    "[lambda_1, lambda_2, tau], kernel = tp.mcmc.sample_chain(\n",
    "    num_results=100000,\n",
    "    num_burnin_steps=10000,\n",
    "    current_state=state,\n",
    "    kernel=kernel,\n",
    ")\n",
    "tau = tf.floor(tau * tf.to_float(tf.size(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = tf.Session()\n",
    "\n",
    "session.run([\n",
    "    tf.global_variables_initializer(),\n",
    "    tf.local_variables_initializer(),\n",
    "])\n",
    "\n",
    "[\n",
    "    lambda_1_,\n",
    "    lambda_2_,\n",
    "    tau_,\n",
    "    kernel_,\n",
    "] = session.run([\n",
    "    lambda_1,\n",
    "    lambda_2,\n",
    "    tau,\n",
    "    kernel,\n",
    "])\n",
    "    \n",
    "print('Acceptance rate: {}'.format(kernel_.inner_results.is_accepted.mean()))\n",
    "print('Final step size: {}'.format(kernel_.inner_results.extra.step_size_assign[-100:].mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.figure(figsize=(10, 4))\n",
    "pp.hist(tau_, bins=observation_num, density=True)\n",
    "pp.xticks(np.arange(observation_num))\n",
    "pp.xlim([37, 47])\n",
    "pp.xlabel('Day')\n",
    "pp.ylabel('Posterior density');"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
