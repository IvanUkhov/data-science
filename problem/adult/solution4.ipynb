{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "* [Adult Data Set](https://archive.ics.uci.edu/ml/datasets/Adult) (UCI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as pp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "\n",
    "from common import Dataset, column_variants, load_data\n",
    "from common import compute_confusion, plot_confusion\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([load_data('data/train.csv'),\n",
    "                  load_data('data/test.csv', skiprows=1)])\n",
    "\n",
    "data.dropna(inplace=True)\n",
    "data = data.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "data.info()\n",
    "display(data.head())\n",
    "sb.countplot(data['Income'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dummy(data, column, drop=None, keep=None):\n",
    "    dummies = pd.get_dummies(data[column])\n",
    "    if keep: drop = list(set(dummies.columns) - set(keep))\n",
    "    if drop: dummies.drop(drop, axis=1, inplace=True)\n",
    "    dummies.columns = [column.lower() for column in dummies.columns]\n",
    "    data = data.join(dummies)\n",
    "    data.drop([column], axis=1, inplace=True)\n",
    "    return data\n",
    "\n",
    "data.drop(['FinalSamplingWeight', 'EducationNumber', 'CapitalGain', 'CapitalLoss'],\n",
    "          axis=1, inplace=True)\n",
    "\n",
    "data = make_dummy(data, 'Education')\n",
    "data = make_dummy(data, 'MaritalStatus')\n",
    "data = make_dummy(data, 'NativeCountry', keep=['Mexico', 'United-States'])\n",
    "data = make_dummy(data, 'Occupation')\n",
    "data = make_dummy(data, 'Race')\n",
    "data = make_dummy(data, 'Relationship')\n",
    "data = make_dummy(data, 'Sex')\n",
    "data = make_dummy(data, 'WorkClass')\n",
    "\n",
    "data.info()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {\n",
    "    'Original': Dataset(data),\n",
    "    'Balanced': Dataset(data, balance=True),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "models = {\n",
    "    'Adaptive boosting':\n",
    "        lambda: AdaBoostClassifier(n_estimators=50),\n",
    "    'Decision tree':\n",
    "        lambda: DecisionTreeClassifier(max_depth=10),\n",
    "    'Gaussian naive Bayes':\n",
    "        lambda: GaussianNB(),\n",
    "    'K-nearest neighbors':\n",
    "        lambda: KNeighborsClassifier(n_neighbors=10),\n",
    "    'Logistic regression':\n",
    "        lambda: LogisticRegression(penalty='l2', C=1.0),\n",
    "    'Multi-layer perceptron':\n",
    "        lambda: MLPClassifier(activation='logistic', solver='adam', alpha=0.0001),\n",
    "    'Random forest':\n",
    "        lambda: RandomForestClassifier(n_estimators=50, max_depth=20),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries = []\n",
    "\n",
    "for dataset_name in sorted(datasets.keys()):\n",
    "    for model_name in sorted(models.keys()):\n",
    "        dataset = datasets[dataset_name]\n",
    "        model = models[model_name]()\n",
    "        model.fit(dataset.x_train, dataset.y_train)\n",
    "        y_score = model.predict_proba(dataset.x_test)\n",
    "        y_predicted = [model.classes_[i] for i in np.argmax(y_score, axis=1)]\n",
    "        y_score = y_score[:, list(model.classes_).index(True)]\n",
    "        summary = {\n",
    "            'Dataset': dataset_name,\n",
    "            'Model': model_name,\n",
    "            'Score': model.score(dataset.x_train, dataset.y_train),\n",
    "        }\n",
    "        summary.update(compute_confusion(dataset.y_test, y_predicted, y_score))\n",
    "        summaries.append(summary)\n",
    "        plot_confusion(dataset.y_test, y_predicted, y_score)\n",
    "        pp.suptitle('{} dataset and {} model'.format(dataset_name, model_name))\n",
    "        print('.', end='')\n",
    "\n",
    "head = ['Dataset', 'Model', 'Score']\n",
    "columns = head + sorted(summaries[0].keys() - head)\n",
    "summaries = pd.DataFrame(summaries, columns=columns)\n",
    "\n",
    "summaries.head(len(summaries))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
