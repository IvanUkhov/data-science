{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from scipy.sparse import coo_matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "embedding_size = 8\n",
    "epoch_count = 10\n",
    "sequence_length = 170\n",
    "split_ratio = 7 / 3\n",
    "variety_threshold = 500\n",
    "vocabulary_size = 12000\n",
    "wide_size = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/data.csv')\n",
    "data = data[['description', 'variety', 'price']]\n",
    "data = data[~data['price'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "varieties = data['variety'].value_counts()\n",
    "varieties = varieties[varieties >= variety_threshold].index\n",
    "data = data[data['variety'].isin(varieties.values)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = int(split_ratio / (1 + split_ratio) * len(data))\n",
    "data = data.sample(frac=1, random_state=42)\n",
    "data_train, data_test = data[:split], data[:split]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "encoder.fit(data_train['variety'])\n",
    "variety_count = len(encoder.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variety_wide_train = encoder.transform(data_train['variety']).reshape([-1, 1])\n",
    "variety_wide_test = encoder.transform(data_test['variety']).reshape([-1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = OneHotEncoder()\n",
    "encoder.fit(variety_wide_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variety_wide_train = encoder.transform(variety_wide_train)\n",
    "variety_wide_test = encoder.transform(variety_wide_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(binary=True, dtype=bool, max_features=vocabulary_size)\n",
    "vectorizer.fit(data_train['description']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(descriptions):\n",
    "    return [\n",
    "        [vectorizer.vocabulary_.get(token, 0) for token in analyzer(description)]\n",
    "        for description in descriptions\n",
    "    ]\n",
    "\n",
    "analyzer = vectorizer.build_analyzer()\n",
    "description_deep_train = convert(data_train['description'].values)\n",
    "description_deep_train = preprocessing.sequence.pad_sequences(description_deep_train, maxlen=sequence_length)\n",
    "description_deep_test = convert(data_test['description'].values)\n",
    "description_deep_test = preprocessing.sequence.pad_sequences(description_deep_test, maxlen=sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "description_wide_train = vectorizer.transform(data_train['description'])\n",
    "description_wide_test = vectorizer.transform(data_test['description'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "description_input = layers.Input(shape=(sequence_length,))\n",
    "embedding_layer = layers.Embedding(vocabulary_size, embedding_size, input_length=sequence_length)(description_input)\n",
    "embedding_layer = layers.Flatten()(embedding_layer)\n",
    "deep_output = layers.Dense(1, activation='linear')(embedding_layer)\n",
    "deep_model = models.Model(inputs=description_input, outputs=deep_output)\n",
    "deep_model.compile(loss='mse', optimizer='adam', metrics=['accuracy'])\n",
    "deep_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "description_input = layers.Input(shape=(vocabulary_size,))\n",
    "variety_input = layers.Input(shape=(variety_count,))\n",
    "wide_input = layers.concatenate([description_input, variety_input])\n",
    "wide_layer = layers.Dense(wide_size, activation='relu')(wide_input)\n",
    "wide_output = layers.Dense(1)(wide_layer)\n",
    "wide_model = models.Model(inputs=[description_input, variety_input], outputs=wide_output)\n",
    "wide_model.compile(loss='mse', optimizer='adam', metrics=['accuracy'])\n",
    "wide_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Composite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "composite_input = layers.concatenate([deep_model.output, wide_model.output])\n",
    "composite_output = layers.Dense(1)(composite_input)\n",
    "composite_model = models.Model(inputs=[deep_model.input] + wide_model.input, outputs=composite_output)\n",
    "composite_model.compile(loss='mse', optimizer='adam', metrics=['accuracy'])\n",
    "composite_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "composite_model.fit([description_deep_train] + [description_wide_train, variety_wide_train],\n",
    "                    data_train['price'], epochs=epoch_count, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "composite_model.evaluate([description_deep_test] + [description_wide_test, variety_wide_test],\n",
    "                         data_test['price'], batch_size=batch_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
